{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "## <span style=\"color:blue\"> Computer Vision - Winter 2024\n",
    "\n",
    "## <span style=\"color:blue\"> Exercise 2 </span>\n",
    "**Reichman University**\n",
    "\n",
    "**Lecturer:** Prof. Yael Moses, RUNI\n",
    "\n",
    "**TA:** Eyal Friedman, RUNI\n",
    "\n",
    "**Submission date: 25.1.24** \\\n",
    "Note: In case you need an extension for any reason, you can submit it by 29.1.24. \\\n",
    "No extra extensions will be given.\n",
    "In case you are in miluim - please contact Yael directly.\n",
    "\n",
    "**Your name: [Your Name]**\n",
    "**Your ID: [Your ID]**\n",
    "\n",
    "In this exercise, you will implement: \n",
    "    \n",
    "1. Straight line detection from image features using  Hough Transform.\n",
    "2. Various feature matching.\n",
    "3. Computing depth from rectified images.\n",
    "\n",
    "\n",
    "## <span style=\"color:blue\"> Submission guidelines:</span>\n",
    "\n",
    "1. Your **zip** file should include the following files only:\n",
    "   - ex2.ipynb\n",
    "   - images you use that were not given \n",
    "2. You should use Jupyter Notebook.\n",
    "3. Name the zip file **'ex2_ID_ID.zip'** and **do not** include any additional directories.\n",
    "4. Submit using *Moodle*.\n",
    "5. Submit on time!\n",
    "6. You can submit this assignment in pairs (no triplets).\n",
    "   * In the case of pair submission, both IDs and names should be added to the notebook.\n",
    "   * One should submit the homework, and the other should submit a simple text file named: ID_ID.txt and nothing else.\n",
    "   *Please make sure that your collaborator submits the HW.\n",
    "\n",
    "## Read the following instructions carefully:\n",
    "\n",
    "1. Write **efficient vectorized** code. When you think you cannot use vectorized code, give a short explanation of why.\n",
    "2. You are responsible for the correctness of your code and should add as many tests as you see fit. Do not submit your tests unless requested.\n",
    "3. Use `Python 3` and `numpy 1.18.5` or above. Changes to the configuration we provided are at your own risk. Before submitting the exercise, restart the kernel and run the notebook from start to finish to make sure everything works.\n",
    "4. You are allowed to use functions and methods from the [Python Standard Library](https://docs.python.org/3/library/) and [numpy](https://www.numpy.org/devdocs/reference/) only. Any other imports are forbidden unless provided by us.\n",
    "5. Your code must run without errors. Note, **Code that fails to run will not be graded.**\n",
    "6. Document your code properly.\n",
    "7. Go over Warmup Python - you can find relevant python functions that will make your life easier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Honor Code:\n",
    "The assignment is a basic tool for learning the material. You can probably find the solution on the web (including ChatGPT), however, you will not learn what you should learn from it. In addition, since we give grades on the assignment, using existing solutions will be considered dishonest.\n",
    "In particular, you are not allowed to copy or use any code that solve the tasks. \n",
    "You are more than welcome to talk with your friends, but you are not allowed to give your code or answers and you are not allowed to use their code or answers. \n",
    "Remember – you take this course in order to learn.\n",
    "\n",
    "## I declair that I respcet the honor code:\n",
    "<span style=\"color:red\"> Add here your names: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:  3.10.11\n",
      "Numpy version:  1.23.5\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "# This opens an inteactive figure - use it in part B\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import argparse\n",
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from scipy.ndimage import maximum_filter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.linalg import null_space\n",
    "\n",
    "# This specifies the way plots behave in jupyter notebook\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (8.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "\n",
    "import platform\n",
    "print(\"Python version: \", platform.python_version())\n",
    "print(\"Numpy version: \", np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## <span style=\"color:blue\">Section A: Detect a Straight Lines\n",
    "\n",
    "In this part you will use the set of edge points to detect straight lines in an image.\\\n",
    "The input will consist of edge points computed by the Canny edge detector - you can use the implementation of CV2, which is demonstrated below.\\\n",
    "    The output will be a set of straight lines in the image. There are two main methods to compute straight lines from such input: the Hough transform and RANSAC. You will implement the Hough transform.\n",
    "\n",
    "    \n",
    "\n",
    "**Hough transform**\\\n",
    "Every 2D line, $\\ell$,   can be represented by 2 parameters:  $r$ and $\\theta$ where all points on the lines satisfy $r=(xcos\\theta, ysin\\theta)$. \n",
    "Let $P_0=(x_0,y_0)$ be the intersection of a normal to $\\ell$ from the origin.\n",
    "The distance between $P_0$ and the origin is given by $r$ and the angle between the normal and the $x$ axis is given by $\\theta$. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your goal:**     Write the following function\\\n",
    "Straight_lines(image_file, res_r, res_orient,min_number_points,display,  ...)\\\n",
    "    You can add any other parameters that you need.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " To do so, you need also to define the following functions. \n",
    " You can add parameters to the functions, as long as you  provide clear explanations of their roles and purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retain_max_in_neighborhood(matrix, radius):\n",
    "    \"\"\"\n",
    "    Retain the maximum value in each neighborhood defined by a circular mask.\n",
    "    \"\"\"\n",
    "    # Create a binary mask to define the circular neighborhood\n",
    "    neighborhood_mask = np.zeros((2 * radius + 1, 2 * radius + 1), dtype=bool)\n",
    "    y, x = np.ogrid[-radius:radius+1, -radius:radius+1]\n",
    "    neighborhood_mask = x**2 + y**2 <= radius**2\n",
    "\n",
    "    # Use scipy's maximum filter to get the local maximum in each circular neighborhood\n",
    "    max_values = maximum_filter(matrix, footprint=neighborhood_mask)\n",
    "\n",
    "    # Set all values to zero except for the maximum value in each neighborhood\n",
    "    result = np.where(matrix == max_values, matrix, 0)\n",
    "\n",
    "    return result\n",
    "\n",
    "def show1Image(base):\n",
    "    f = plt.figure()\n",
    "    f.add_subplot(1, 2, 1)\n",
    "    plt.imshow(base)\n",
    "    ax = plt.gca()\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.xaxis.set_major_locator(MultipleLocator(int(base.shape[0]/10)))\n",
    "    ax.yaxis.set_major_locator(MultipleLocator(int(base.shape[0]/10)))\n",
    "    ax.grid(which='major', linestyle='--', linewidth=0.5)\n",
    "    ax.grid(which='minor', linestyle='--', linewidth=0.25)\n",
    "\n",
    "def show2Images(base, conv):\n",
    "    f = plt.figure()\n",
    "    plt.tight_layout()\n",
    "    f.add_subplot(1, 2, 1)\n",
    "    plt.imshow(base)\n",
    "    ax = plt.gca()\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.xaxis.set_major_locator(MultipleLocator(int(base.shape[0]/10)))\n",
    "    ax.yaxis.set_major_locator(MultipleLocator(int(base.shape[0]/10)))\n",
    "    ax.grid(which='major', linestyle='--', linewidth=0.5)\n",
    "    ax.grid(which='minor', linestyle='--', linewidth=0.25)\n",
    "    f.add_subplot(1, 2, 2)\n",
    "    plt.imshow(conv)\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.xaxis.set_major_locator(MultipleLocator(int(base.shape[0]/10)))\n",
    "    ax.yaxis.set_major_locator(MultipleLocator(int(base.shape[0]/10)))\n",
    "    ax.grid(which='major', linestyle='--', linewidth=0.5)\n",
    "    ax.grid(which='minor', linestyle='--', linewidth=0.25)\n",
    "    plt.show(block=True)\n",
    "\n",
    "def show3Images(img1, img2, img3):\n",
    "        \n",
    "    # Create a figure\n",
    "    fig = plt.figure()  # Adjust the figsize as needed\n",
    "\n",
    "    # Add three subplots in a horizontal layout\n",
    "    ax1 = fig.add_subplot(1, 3, 1)\n",
    "    ax2 = fig.add_subplot(1, 3, 2)\n",
    "    ax3 = fig.add_subplot(1, 3, 3)\n",
    "\n",
    "    # Plot the images in the subplots\n",
    "    ax1.imshow(img1, cmap='gray')  # Replace 'gray' with the desired colormap\n",
    "    ax1.grid(which='major', linestyle='--', linewidth=0.5)\n",
    "    ax1.grid(which='minor', linestyle='--', linewidth=0.25)\n",
    "    ax1.xaxis.tick_top()\n",
    "    ax1.xaxis.set_major_locator(MultipleLocator(int(img3.shape[0]/8)))\n",
    "    ax1.yaxis.set_major_locator(MultipleLocator(int(img3.shape[0]/8)))\n",
    "\n",
    "    ax2.imshow(img2, cmap='gray')\n",
    "    ax2.grid(which='major', linestyle='--', linewidth=0.5)\n",
    "    ax2.grid(which='minor', linestyle='--', linewidth=0.25)\n",
    "    ax2.xaxis.tick_top()\n",
    "    ax2.xaxis.set_major_locator(MultipleLocator(int(img3.shape[0]/8)))\n",
    "    ax2.yaxis.set_major_locator(MultipleLocator(int(img3.shape[0]/8)))\n",
    "\n",
    "    ax3.imshow(img3, cmap='gray')\n",
    "    ax3.grid(which='major', linestyle='--', linewidth=0.5)\n",
    "    ax3.grid(which='minor', linestyle='--', linewidth=0.25)\n",
    "    ax3.xaxis.tick_top()\n",
    "    ax3.xaxis.set_major_locator(MultipleLocator(int(img3.shape[0]/8)))\n",
    "    ax3.yaxis.set_major_locator(MultipleLocator(int(img3.shape[0]/8)))\n",
    "\n",
    "    # Adjust layout for better spacing\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_hough_lines(image, edge_image, num_rhos, num_thetas, bin_threshold):\n",
    "    # accumulator, thetas, rhos are calculated for entire image, Now return only the ones which have higher votes.\n",
    "    # if required all can be returned here, the below code could be post processing done by the user.\n",
    "    pass\n",
    "\n",
    "\n",
    "# Input: a set of edge points (or corners), and the resolution of the distance and angles.\n",
    "\n",
    "# output: the Hough matrix (H) containing votes for lines represented by r and θ.\n",
    "\n",
    "\n",
    "def H_matrix(L_points, resolution_r, resolution_ang):\n",
    "    # image size\n",
    "    img_height, img_width = L_points.shape[:2]\n",
    "    img_height_half = img_height / 2\n",
    "    img_width_half = img_width / 2\n",
    "    # Rho and Theta ranges\n",
    "    diag_len = np.sqrt(np.square(img_height) + np.square(img_width))\n",
    "    # Thetas is bins created from 0 to 180 degree with increment of the provided dtheta\n",
    "    thetas = np.arange(0, 180, step=resolution_ang)\n",
    "    # Rho ranges from -diag_len to diag_len where diag_len is the diagonal length of the input image\n",
    "    rhos = np.arange(-diag_len, diag_len, step=resolution_r)\n",
    "    # Calculate Cos(theta) and Sin(theta) it will be required later on while calculating rho\n",
    "    cos_thetas = np.cos(np.deg2rad(thetas))\n",
    "    sin_thetas = np.sin(np.deg2rad(thetas))\n",
    "    # Hough accumulator array of theta vs rho, (rho,theta)\n",
    "    accumulator = np.zeros((len(rhos), len(thetas)))\n",
    "    # Hough Space plot for the image.\n",
    "    figure = plt.figure()\n",
    "    hough_plot = figure.add_subplot()\n",
    "    hough_plot.set_facecolor((0, 0, 0))\n",
    "    hough_plot.title.set_text(\"Hough Space\")\n",
    "\n",
    "    # Iterate through pixels and if non-zero pixel process it for hough space\n",
    "    for y in range(img_height):\n",
    "        for x in range(img_width):\n",
    "            if L_points[y][x] != 0:  # white pixel\n",
    "                edge_pt = [y - img_height_half, x - img_width_half]\n",
    "                hough_rhos, hough_thetas = [], []\n",
    "                # Iterate through theta ranges to calculate the rho values\n",
    "                for theta_idx in range(len(thetas)):\n",
    "                    # Calculate rho value\n",
    "                    rho = (edge_pt[1] * cos_thetas[theta_idx]) + \\\n",
    "                        (edge_pt[0] * sin_thetas[theta_idx])\n",
    "                    theta = thetas[theta_idx]\n",
    "                    # Get index of nearest rho value\n",
    "                    rho_idx = np.argmin(np.abs(rhos - rho))\n",
    "                    # increment the vote for (rho_idx,theta_idx) pair\n",
    "                    accumulator[rho_idx][theta_idx] += 1\n",
    "                    # Append values of rho and theta in hough_rhos and hough_thetas respectively for Hough Space plotting.\n",
    "                    hough_rhos.append(rho)\n",
    "                    hough_thetas.append(theta)\n",
    "\n",
    "                # Plot Hough Space from the values\n",
    "                hough_plot.plot(hough_thetas, hough_rhos,color=\"white\", alpha=0.05)\n",
    "    return accumulator, thetas, rhos\n",
    "\n",
    "\n",
    "# Input: The Hough matrix $H$, and a threshold for the number of minimal points on the line.\n",
    "\n",
    "# output a list of triplets:  $(r, \\Theta, num_points)$ where\n",
    "\n",
    "# num_points is the number of points on that line.\n",
    "\n",
    "\n",
    "\n",
    "def list_lines(H, th, img_shape, rhos, thetas):\n",
    "    # th - number of minimal points on the line\n",
    "    out_lines = []\n",
    "    for y in range(H.shape[0]):\n",
    "        for x in range(H.shape[1]):\n",
    "            # If number of votes is greater than bin_threshold provided shortlist it as a candidate line\n",
    "            if H[y][x] > th:\n",
    "                rho = rhos[y]\n",
    "                theta = thetas[x]\n",
    "\n",
    "                # a and b are intercepts in x and y direction\n",
    "                a = np.cos(np.deg2rad(theta))\n",
    "                b = np.sin(np.deg2rad(theta))\n",
    "\n",
    "                x0 = (a * rho) + img_shape[1] / 2\n",
    "                y0 = (b * rho) + img_shape[0] / 2\n",
    "\n",
    "                # Get the extreme points to draw the line\n",
    "                x1 = int(x0 + 1000 * (-b))\n",
    "                y1 = int(y0 + 1000 * (a))\n",
    "                x2 = int(x0 - 1000 * (-b))\n",
    "                y2 = int(y0 - 1000 * (a))\n",
    "                out_lines.append((rho, theta, x1, y1, x2, y2))\n",
    "    return out_lines\n",
    "\n",
    "\n",
    "# Display the detected lines in red - overlaid the original image\n",
    "# Note: one way to do is, is to add the red lines to the image, and then display it\n",
    "def display_lines(im, list_lines):\n",
    "    output_img = im.copy()\n",
    "    for line in list_lines:\n",
    "        output_img = cv2.line(output_img, (line[2], line[3]), (line[4], line[5]), (255,0, 0), 1)\n",
    "    plt.imshow(output_img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgpath = 'cube1.jpg'\n",
    "num_rho = 180\n",
    "num_theta = 180\n",
    "bin_threshold = 100\n",
    "lines_are_white = True\n",
    "\n",
    "input_img = cv2.imread(imgpath)\n",
    "show1Image(input_img)\n",
    "\n",
    "#Edge detection on the input image\n",
    "edge_image = cv2.cvtColor(input_img, cv2.COLOR_BGR2GRAY)\n",
    "ret, edge_image = cv2.threshold(edge_image, 120, 255, cv2.THRESH_BINARY_INV)\n",
    "edge_image = cv2.Canny(edge_image, 100, 200)\n",
    "\n",
    "show1Image(edge_image)\n",
    "\n",
    "print (\"Detecting Hough Lines Started!\")\n",
    "line_img, lines, accumulator, thetas, rhos = find_hough_lines(input_img, edge_image, num_rho, num_theta, bin_threshold)\n",
    "print(lines)\n",
    "\n",
    "show1Image(line_img)\n",
    "# print(peak_votes(accumulator, thetas, rhos))\n",
    "        \n",
    "print (\"Detecting Hough Lines Complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now use the above functions to implement \n",
    "def straight_lines(image_file, res_r, res_orient,min_number_points,display,  ...)()\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is an example of how to draw a red line\n",
    "# between (x1, y1) and (x2, y2) on a gray level image, img\n",
    "# The first step is to create a color image from img.\n",
    "\n",
    "imC=np.dstack([img,img,img])   # a gray level image that is saved as a color image\n",
    "cv2.line(img, (x1, y1), (x2, y2), (255,0,0), thickness)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is an example of how to use the CV2 Canny edge detector.\n",
    "# You can play with the parameters to achieve desired results.\n",
    "# Note that this implementation does not include smoothing with a Gaussian filter :(\n",
    "\n",
    "\n",
    "img = cv2.imread('.\\images\\Sudoku.PNG', cv2.IMREAD_GRAYSCALE)\n",
    "assert img is not None, \"file could not be read, check with os.path.exists()\"\n",
    "\n",
    "edges = cv2.Canny(img,250,500,5)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (16.0, 16.0) \n",
    "f, ((ax1, ax2)) = plt.subplots(1, 2, sharex='col', sharey='row')\n",
    "\n",
    "ax1.imshow(img), ax1.set_title('Original Image')\n",
    "ax2.imshow(edges), ax2.set_title('Edge Image')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply and answer**\n",
    "\n",
    "1. Generate a synthetic image to test your Hough Transform algorithm. \n",
    "   * Submit the image as well as the results.\n",
    "\n",
    "\n",
    "2. Apply your algorithm to the following images: Crosswalk, linesOnTheRoadGray, Sudoku. \n",
    "   * Choose a set of parameters so the results looks fine.\n",
    "   * You may need different parameters for each image.\n",
    "   * Display the results.\n",
    "   \n",
    "   \n",
    "3. **Answer**: How the resolutions of $r$ and $\\theta$ affect the results?\n",
    "   * Display images that demonstarte your answer.\n",
    "   \n",
    "   \n",
    "4. Choose one image and **answer**: how many straight lines did you find with more than 50 points?\n",
    "   * Display these lines on the image.\n",
    "   \n",
    "   \n",
    "5. Suggest an algorithm to compute the length of the line in the image.\n",
    "   * Describe the algorithm without implementing it.\n",
    "   \n",
    "   \n",
    "6. Suggest three applications to use the results of straight line detection in an image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Section B: Patch Matching\n",
    "**In this part you will match intrest points between two images.**\n",
    "    \n",
    "    \n",
    "There are two functions between vectors $v$ and $u$ that you will consider:\\\n",
    "a. Distance: Sum of Square Distance (SSD). That is,  $SSD(u,v)=(u-v)\\cdot(u-v)$.\\\n",
    "b. Similarity: Normalized Cross Correlation (NCC). That is,  $NCC(u,v)={u\\cdot v \\over ||u||||v||}$.\n",
    "\n",
    "\n",
    "The patch descriptor:\n",
    "1. A vector with the pixels' grey level.\n",
    "2. An histogram (30 bins) of the pixels' grey level.\n",
    "3. A vector with the pixels' strength gradient.\n",
    "4. An histogram (30 bins) of the pixels' strength of gradient. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement the following functions:\n",
    "\n",
    "# Input: two vectors\n",
    "# Output distance (scalar) between two patches\n",
    "\n",
    "def  SSD(patch_descr_1, patch_descr_2) ()\n",
    "...\n",
    "\n",
    "# Input: two vectors\n",
    "# Output normalized cross correlation  (scalar) between two patches\n",
    "\n",
    "def NCC(patch_descr_1, patch_descr_2) ()\n",
    "..\n",
    "\n",
    "# Output a descriptor vector \n",
    "# im is an image, p is a pixel, size is the patch size.\n",
    "# You can use the histogram function of open cv or numpy\n",
    "\n",
    "def patch_from_im(im,p,size) ()\n",
    "...\n",
    "\n",
    "def hist_patch_im(im,p,size) ()\n",
    "...\n",
    "\n",
    "def  gradient(im,p,size) ()\n",
    "...\n",
    "\n",
    "def  hist_gradient(im,p,size) ()\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply and answer**\n",
    "\n",
    "\n",
    "1. Compute the corners in a pair of images (e.g., view0.tif and view6.tif).\n",
    "\n",
    "\n",
    "2. Choose up to 2000 of the strongest corners (you can use fewer) and find a matching between the corners in the pair of images.\n",
    "    * Choose a fixed scale and use it to define a patch around each corner.\n",
    "    * To display the matching, use the function cv2.hconcat([im1, im2]), which concatenates two images horizontally, and then draw lines as described in the previous section.\n",
    "\n",
    "\n",
    "3. Use up to 4000 of the strongest corners (you can use fewer) in each image and use the matching only between corners that have approximately the same y-coordinate (up to a threshold).\n",
    "\n",
    "\n",
    "4. Study the ratio for matching between the best and the second best match. \n",
    "   * You can use NCC or SSD on the desriptor of your choice.\n",
    "   * You can use without (2) or with (3) the $y$-coordinate constraint.\n",
    "   * **Present** examples that demonstrate the effectiveness of using the ratio.\n",
    "\n",
    "\n",
    "5. Study the differences between the different descriptors and also the use of SSD or NCC. \n",
    "   * **Present**  examples that demonstrate your findings.\n",
    "\n",
    "\n",
    "6. Identify incorrect pairs of matched points.\\\n",
    "   **Mark and display** for cases (2), (3), and (4) a pair of incorrectly matched points.\\\n",
    "   **Answer**\\\n",
    "   a. In which of the 3 cases are there more incorrect matches?\\\n",
    "   b. What may be the reason for the incorrect matches?\n",
    "\n",
    "\n",
    "7. In which regions of the scene were most of the reliable matches found?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation details**\n",
    "\n",
    "You can compute the NCC or SSD patch by patch. This will probably takes longer than using matrix multiplications of numpy.\n",
    "\n",
    "Using matrices:\n",
    "1. Generate a 1D vector from each descriptor.\n",
    "\n",
    "\n",
    "2. Generate a matrix $M_1$:  rows  are  the descriptors of img1.\n",
    "\n",
    "\n",
    "3. Generate a matrix $M_2$:  columns  are  the descriptors of img2.\n",
    "\n",
    "\n",
    "4. $M=np.matmul(M_1,M_2)$\\\n",
    "   * what is the value in $M(i,j)$?\n",
    "\n",
    "\n",
    "5. Think of how to normalize the set of patches, if you need.\n",
    "   * You can consider using the function np.diag(np.diag(matrix)) as part of your solution.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Section C\n",
    "\n",
    "In this part you will compute the 3D structure from a pair of rectified images.\n",
    "\n",
    "\n",
    "**The input:**\\\n",
    "    **a.** A pair of two rectified images, $im1$ and $im2$ (*view1.png and view5.png*).\\\n",
    "    **b.** A window size $(s_x,s_y$).\\\n",
    "    **c.** Disparity range $(d_{min},d_{max})$ (see below).\n",
    "\n",
    "**The output:**\\\n",
    "**a.** A matrix, $D$ with the disparity map for the left image.\\\n",
    "**b.** Three matrices X, Y, Z with the x, y, z-coordinates of each pixel in the left image. Assign zero for pixels for which the disparity was not computed.\n",
    "\n",
    "Note: all matrices $im1, im2, D, X, Y, Z$ have the same dimensions.\n",
    "\n",
    "**Instructions:**\n",
    "1. Read the two images view1.png and view5.png, and apply the functions to this pair of images.\n",
    "\n",
    "   \n",
    "2. Write a function that receives two rectified images, and compute dense matching along lines with the same $y-$coordinate.\n",
    "    * You can use any of the descriptors from Part B. The patch size should be given as a parameter.\n",
    "    * Use NCC for the similarity measure.\n",
    "    * You can assume that you are given the range of disparities, $d_{min},d_{max}$ as an input. \\\n",
    "      For example, if  $(d_{min},d_{max})=(20,120)$ it follows that the corresponding point of  $(x,y)$ in the left image, is in the range $(x-120:x-20,y)$ in the right image.\n",
    "    * Consider matching with and without order preseving. \n",
    "    * List all parameters you use for your function.\n",
    "    * Hints on efficient implementation is given below.\n",
    "    \n",
    "        \n",
    "3. Write a function that computes the disparity based on (2).  \n",
    "    * **Display** the disparity $D$ map as an image\n",
    "   \n",
    "  \n",
    "4. Compute the depth map using the disparity.\n",
    "    * Assume that scaled focal lengths ($f$ in the presentation) are $α_x=α_y=1$.\n",
    "    * The distance between the cameras is 160mm.\n",
    "    * Add to your disparity depth map the value 100, since images were cropped. \n",
    "    * Display the disparity as an image. \n",
    "   \n",
    "\n",
    "4. Compute the matrices X,Y,Z\n",
    "    * Present the object given by X,Y,Z using a 3D plot.\n",
    "\n",
    "\n",
    "**Implementation**\\\n",
    "You can implement the computation of the similarity and the disparity directly by an exhaustive search. You can also use the idea from Part B. A more efficeint and elegant way is described below. You are welcome to try, but you do not have to.\n",
    "\n",
    "\n",
    "General idea for the **vectorized solution** (you will need to fill in the gaps):\n",
    "* Let $w$ and $h$ we the width and the height of the image respectively.\n",
    "* First assume that the patch size is $1\\times s_y$.\n",
    "* Given two $1\\times w$ rows, $r_1$ and $r_2$ which are corresponding rows of the two images. Compute the $w\\times w$ matrix $R12=r_1^T r_2$. The values $R12(i,j)$ consists of the product of two pixels $r_1(i)\\cdot r_2(j)$.\n",
    "* Convolve $R12$ with the $s_y\\times s_y$ identity matrix, $I_{s_y}$ : $Corr=I_{s_y}*R12$. The value $Corr(i,j)$ is the correlation of the $s_y$ 1D neighborhood of $r_1(i)$ and  $r_2(j)$.\n",
    "* Think about how to normalize this correlation using the same idea. Hint, use the computation of $I_{s_y}*(r_1^Tr_1)$ and $I_{s_y}*(r_2^Tr_2)$.\n",
    "* For a general patch size, $s_x\\times s_y$, define $R$ to be 3D matrix of size $h\\times w\\times s_x$ and use 3D mask for the convolution.\n",
    "* Up to here, we considered each row seperately, hence we need to loop over all rows of the images. There is a way to do it without looping, but not in this assignemnt :)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
